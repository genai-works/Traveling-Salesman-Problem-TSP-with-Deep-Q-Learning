{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  1. Import Libraries\n"
      ],
      "metadata": {
        "id": "JVPvYzVu029J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7nQt43y01qO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for Neural Networks (PyTorch), Randomness, and Visualization\n",
        "import numpy as np           # For mathematical operations and arrays\n",
        "import torch                 # PyTorch for building neural networks\n",
        "import torch.nn as nn        # Neural network layers\n",
        "import torch.optim as optim   # Optimizers for updating the weights\n",
        "import random                # Randomness for city generation and action selection\n",
        "import matplotlib.pyplot as plt  # For plotting and visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Traveling Salesman Problem (TSP)\n",
        "\n",
        "The Traveling Salesman Problem (TSP) asks the question: \"Given a list of cities and the distances between them, what is the shortest possible route that visits each city exactly once and returns to the origin city?\"\n",
        "\n",
        "We will solve this using Deep Reinforcement Learning with a Q-learning-based agent.\n"
      ],
      "metadata": {
        "id": "2NddcbJA08DA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. City Class for TSP\n"
      ],
      "metadata": {
        "id": "jsB2_Z5B1JDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# City Class\n",
        "# This class represents each city in the TSP problem by its coordinates (x, y).\n",
        "# It also includes a method to calculate the Euclidean distance to another city.\n",
        "\n",
        "class City:\n",
        "    def __init__(self, x, y):\n",
        "        \"\"\"\n",
        "        Initialize the city with its x and y coordinates.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def distance_to(self, city):\n",
        "        \"\"\"\n",
        "        Compute the Euclidean distance between this city and another city.\n",
        "        Euclidean distance formula: sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
        "        \"\"\"\n",
        "        return np.sqrt((self.x - city.x)**2 + (self.y - city.y)**2)\n"
      ],
      "metadata": {
        "id": "bpMmDBXf07r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Deep Q-Network (DQN) Definition\n"
      ],
      "metadata": {
        "id": "zooYpDrX1L8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model for Q-learning\n",
        "# This model will approximate the Q-values for each possible action (city to visit next).\n",
        "# The network has three fully connected layers with ReLU activations.\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the Deep Q-Network with input_size representing the number of cities\n",
        "        and output_size representing the number of possible actions (next city to visit).\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)   # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)           # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, output_size)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass through the neural network.\n",
        "        Input is the current state (cities visited so far), and output is Q-values.\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))   # Apply ReLU activation to first hidden layer\n",
        "        x = torch.relu(self.fc2(x))   # Apply ReLU activation to second hidden layer\n",
        "        return self.fc3(x)   # Output layer provides Q-values for each action (city)\n"
      ],
      "metadata": {
        "id": "TjHMIri41MxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. TSP Agent Setup (Reinforcement Learning)\n"
      ],
      "metadata": {
        "id": "Sex4xHdU1OYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TSP Agent for Reinforcement Learning\n",
        "# This agent interacts with the environment, makes decisions, stores experiences, and learns from them.\n",
        "# The agent uses Q-learning to find the optimal tour that minimizes the travel distance.\n",
        "\n",
        "class TSPAgent:\n",
        "    def __init__(self, num_cities, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=0.001):\n",
        "        \"\"\"\n",
        "        Initialize the agent with hyperparameters for Q-learning:\n",
        "        - gamma: discount factor (how much future rewards are considered)\n",
        "        - epsilon: initial exploration rate (for epsilon-greedy strategy)\n",
        "        - epsilon_decay: how quickly exploration rate decays\n",
        "        - epsilon_min: minimum exploration rate\n",
        "        - lr: learning rate for optimizer\n",
        "        \"\"\"\n",
        "        self.num_cities = num_cities\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Exploration rate for epsilon-greedy policy\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.memory = []  # Memory buffer to store experiences\n",
        "        self.batch_size = 32  # Number of experiences to sample for training\n",
        "        self.memory_capacity = 10000  # Max capacity of memory buffer\n",
        "        self.q_network = DQN(num_cities, num_cities)  # Initialize the Q-network\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)  # Optimizer for training\n",
        "        self.loss_fn = nn.MSELoss()  # Mean Squared Error loss for training\n"
      ],
      "metadata": {
        "id": "xzq1NyBN1RIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Agent: Experience Storage and Action Selection\n"
      ],
      "metadata": {
        "id": "o95qdvbQ1Sp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store the agent's experience (state, action, reward, next_state, done) in memory for experience replay.\n",
        "        If memory exceeds its capacity, the oldest experience is removed.\n",
        "        \"\"\"\n",
        "        if len(self.memory) > self.memory_capacity:\n",
        "            self.memory.pop(0)  # Remove the oldest experience if memory is full\n",
        "        self.memory.append((state, action, reward, next_state, done))  # Add new experience\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose an action (next city to visit) using epsilon-greedy strategy.\n",
        "        - With probability epsilon, choose a random action (exploration).\n",
        "        - Otherwise, choose the action with the highest Q-value (exploitation).\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:  # Exploration\n",
        "            return random.randint(0, self.num_cities - 1)  # Choose a random city to visit\n",
        "        else:  # Exploitation\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor\n",
        "            with torch.no_grad():  # No gradient calculation needed\n",
        "                q_values = self.q_network(state_tensor)  # Get Q-values for current state\n",
        "            return torch.argmax(q_values).item()  # Choose the action with the highest Q-value\n"
      ],
      "metadata": {
        "id": "-5aI3Ayd1U56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training the Agent (Q-Learning)\n"
      ],
      "metadata": {
        "id": "5_6vpi7L1Wb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the Q-network using the experiences stored in memory.\n",
        "        For each experience, update the Q-value towards the target value:\n",
        "        Q_target = reward + gamma * max(Q(next_state)) if not done.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return  # Don't train if there aren't enough experiences in memory\n",
        "\n",
        "        # Sample a batch of experiences randomly from memory\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)  # Convert next state to tensor\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * torch.max(self.q_network(next_state_tensor))  # Target Q-value\n",
        "\n",
        "            q_value = self.q_network(state_tensor)[0, action]  # Current Q-value for the action\n",
        "            loss = self.loss_fn(q_value, torch.tensor(target))  # Compute the loss (target - predicted)\n",
        "\n",
        "            # Perform backpropagation to update network weights\n",
        "            self.optimizer.zero_grad()  # Reset gradients\n",
        "            loss.backward()  # Compute gradients\n",
        "            self.optimizer.step()  # Update network weights\n",
        "\n",
        "        # Decay epsilon for less exploration over time\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "D8IFjoS81YrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. TSP Environment Class (Simulating the Problem)\n"
      ],
      "metadata": {
        "id": "Et0p7CNa1aOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TSP Environment\n",
        "# This environment simulates the cities and their connections.\n",
        "# It interacts with the agent and provides rewards based on actions (city visits).\n",
        "\n",
        "class TSPEnvironment:\n",
        "    def __init__(self, cities):\n",
        "        \"\"\"\n",
        "        Initialize the environment with a list of cities.\n",
        "        It also tracks the current state (cities visited), total distance, and the current city.\n",
        "        \"\"\"\n",
        "        self.cities = cities\n",
        "        self.num_cities = len(cities)\n",
        "        self.current_state = []  # The current state will store which cities have been visited\n",
        "        self.current_city = None  # The current city the agent is located in\n",
        "        self.visited = []  # List of visited cities\n",
        "        self.total_distance = 0  # Total distance traveled by the agent\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment for a new episode.\n",
        "        The agent starts from a random city, and no cities have been visited yet.\n",
        "        \"\"\"\n",
        "        self.current_state = np.zeros(self.num_cities)  # All cities are unvisited initially\n",
        "        self.current_city = random.randint(0, self.num_cities - 1)  # Start from a random city\n",
        "        self.visited = [self.current_city]  # Mark the start city as visited\n",
        "        self.total_distance = 0  # Reset total distance traveled\n",
        "        return self.current_state  # Return the initial state (unvisited cities)\n"
      ],
      "metadata": {
        "id": "JAh18sgv1cM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Step Function for Environment (Reward Calculation)\n"
      ],
      "metadata": {
        "id": "rX304IIk1d0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment by visiting a city (the action).\n",
        "        - If the agent revisits a city, it gets a negative reward.\n",
        "        - Otherwise, the reward is the negative distance traveled to the new city.\n",
        "        - The episode ends when all cities are visited (done=True).\n",
        "        \"\"\"\n",
        "        if action in self.visited:\n",
        "            reward = -10  # Negative reward for revisiting a city\n",
        "        else:\n",
        "            reward = -self.cities[self.current_city].distance_to(self.cities[action])  # Negative distance as reward\n",
        "            self.total_distance += abs(reward)  # Accumulate the total distance\n",
        "            self.visited.append(action)  # Mark the city as visited\n",
        "\n",
        "        self.current_city = action  # Move to the new city\n",
        "        self.current_state[action] = 1  # Update the state to mark this city as visited\n",
        "\n",
        "        done = len(self.visited) == self.num_cities  # Check if all cities have been visited\n",
        "        if done:\n",
        "            # Add the return trip to the starting city\n",
        "            reward -= self.cities[self.current_city].distance_to(self.cities[self.visited[0]])\n",
        "            self.total_distance += abs(reward)  # Include the distance to return to the start\n",
        "\n",
        "        return self.current_state, reward, done  # Return the next state, reward, and done flag\n"
      ],
      "metadata": {
        "id": "nYBf_F771flP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Training Function for the TSP Agent\n"
      ],
      "metadata": {
        "id": "lRRIW-WK1iNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the agent for multiple episodes of interaction with the environment\n",
        "def train_tsp_agent(cities, episodes=1000):\n",
        "    \"\"\"\n",
        "    Train the TSP agent using Q-learning for a given number of episodes.\n",
        "    The agent interacts with the environment, learns from its experiences, and improves its policy.\n",
        "    \"\"\"\n",
        "    env = TSPEnvironment(cities)  # Create the environment with the cities\n",
        "    agent = TSPAgent(num_cities=len(cities))  # Initialize the Q-learning agent\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()  # Reset the environment at the beginning of each episode\n",
        "        done = False\n",
        "        total_reward = 0  # Track total reward (negative total distance)\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)  # Agent chooses the next city (action)\n",
        "            next_state, reward, done = env.step(action)  # Environment responds with next state and reward\n",
        "            agent.store_experience(state, action, reward, next_state, done)  # Store experience for replay\n",
        "            state = next_state  # Move to the next state\n",
        "            total_reward += reward  # Accumulate the reward (minimizing negative distance)\n",
        "            agent.train()  # Train the agent based on stored experiences\n",
        "\n",
        "        print(f\"Episode {episode+1}, Total Reward: {total_reward}, Total Distance: {env.total_distance}\")\n",
        "\n",
        "\n",
        "# Plotting the solution learned by the agent\n",
        "def plot_solution(cities, path):\n",
        "    \"\"\"\n",
        "    Visualize the final path learned by the agent. The cities will be connected based on the path found.\n",
        "    \"\"\"\n",
        "    x_coords = [city.x for city in path]  # Get x coordinates of the cities\n",
        "    y_coords = [city.y for city in path]  # Get y coordinates of the cities\n",
        "    plt.plot(x_coords + [x_coords[0]], y_coords + [y_coords[0]], 'bo-')  # Plot path and return to start\n",
        "    plt.xlabel('X Coordinate')\n",
        "    plt.ylabel('Y Coordinate')\n",
        "    plt.title('Traveling Salesman Path')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "QFT69iS51jFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Example Execution\n"
      ],
      "metadata": {
        "id": "FXGV6dHm1kgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example execution: Define a set of random cities and train the TSP agent\n",
        "\n",
        "# Generate a random set of cities for the TSP problem\n",
        "num_cities = 10\n",
        "cities = [City(x=random.randint(0, 100), y=random.randint(0, 100)) for _ in range(num_cities)]\n",
        "\n",
        "# Train the Q-learning agent for 500 episodes\n",
        "train_tsp_agent(cities, episodes=500)\n",
        "\n",
        "# Plot the learned solution (after training, path is assumed to be obtained)\n",
        "# plot_solution(cities, learned_path)  # This will visualize the path learned by the agent\n"
      ],
      "metadata": {
        "id": "tgM-R6Kl1qQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}